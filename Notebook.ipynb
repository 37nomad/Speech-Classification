{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1cd1151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import torch\n",
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5ca7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "labels = []\n",
    "audio_files = [\"Audio_Speech_Actors_01-24\", \"Audio_Song_Actors_01-24\"]\n",
    "for main_file in audio_files:\n",
    "    for actor in os.listdir(main_file):\n",
    "        actor_path = os.path.join(main_file,actor)\n",
    "        for audio in os.listdir(actor_path):\n",
    "            if audio.endswith(\".wav\"):\n",
    "                path = os.path.join(actor_path, audio)\n",
    "                paths.append(path)\n",
    "                emotion = int(audio.split(\"-\")[2])\n",
    "                labels.append(emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a69861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of paths:  2452\n",
      "Length of labels:  2452\n",
      "Maximum label:  8\n",
      "Minimum label:  1\n",
      "Data type of paths:  <class 'str'>\n",
      "Data type of lables:  <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of paths: \",len(paths))\n",
    "print(\"Length of labels: \",len(labels))\n",
    "print(\"Maximum label: \",max(labels))\n",
    "print(\"Minimum label: \",min(labels))\n",
    "print(\"Data type of paths: \",type(paths[0]))\n",
    "print(\"Data type of lables: \",type(labels[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23ae220e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Audio_Song_Actors_01-24\\\\Actor_24\\\\03-02-03-02-02-01-24.wav', 3),\n",
       " ('Audio_Song_Actors_01-24\\\\Actor_02\\\\03-02-03-02-02-02-02.wav', 3),\n",
       " ('Audio_Speech_Actors_01-24\\\\Actor_14\\\\03-01-03-01-02-01-14.wav', 3),\n",
       " ('Audio_Speech_Actors_01-24\\\\Actor_17\\\\03-01-07-01-02-01-17.wav', 7),\n",
       " ('Audio_Song_Actors_01-24\\\\Actor_06\\\\03-02-06-01-02-01-06.wav', 6)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(zip(paths, labels))\n",
    "seed = random.Random(42)\n",
    "seed.shuffle(data)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d899006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio_Song_Actors_01-24\\Actor_24\\03-02-03-02-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audio_Song_Actors_01-24\\Actor_02\\03-02-03-02-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audio_Speech_Actors_01-24\\Actor_14\\03-01-03-01...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio_Speech_Actors_01-24\\Actor_17\\03-01-07-01...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio_Song_Actors_01-24\\Actor_06\\03-02-06-01-0...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label\n",
       "0  Audio_Song_Actors_01-24\\Actor_24\\03-02-03-02-0...      3\n",
       "1  Audio_Song_Actors_01-24\\Actor_02\\03-02-03-02-0...      3\n",
       "2  Audio_Speech_Actors_01-24\\Actor_14\\03-01-03-01...      3\n",
       "3  Audio_Speech_Actors_01-24\\Actor_17\\03-01-07-01...      7\n",
       "4  Audio_Song_Actors_01-24\\Actor_06\\03-02-06-01-0...      6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.DataFrame(data, columns=[\"path\",\"label\"])\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66bf93d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2452 entries, 0 to 2451\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   path    2452 non-null   object\n",
      " 1   label   2452 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 38.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57c94b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.318108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.020284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  2452.000000\n",
       "mean      4.318108\n",
       "std       2.020284\n",
       "min       1.000000\n",
       "25%       3.000000\n",
       "50%       4.000000\n",
       "75%       6.000000\n",
       "max       8.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57157e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path     Audio_Song_Actors_01-24\\Actor_24\\03-02-03-02-0...\n",
       "label                                                    3\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecc2e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "df_combined[\"path\"] = df_combined[\"path\"].str.replace(\"\\\\\", \"/\", regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53dcf9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_24/03-02-03-02-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_02/03-02-03-02-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_14/03-01-03-01...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_17/03-01-07-01...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_06/03-02-06-01-0...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label\n",
       "0  Audio_Song_Actors_01-24/Actor_24/03-02-03-02-0...      3\n",
       "1  Audio_Song_Actors_01-24/Actor_02/03-02-03-02-0...      3\n",
       "2  Audio_Speech_Actors_01-24/Actor_14/03-01-03-01...      3\n",
       "3  Audio_Speech_Actors_01-24/Actor_17/03-01-07-01...      7\n",
       "4  Audio_Song_Actors_01-24/Actor_06/03-02-06-01-0...      6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b8d4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = train_test_split(df_combined, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "218ea4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_08/03-01-08-01...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_05/03-01-01-01...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_15/03-02-03-02-0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_05/03-01-08-02...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_20/03-01-02-02...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  label\n",
       "2138  Audio_Speech_Actors_01-24/Actor_08/03-01-08-01...      8\n",
       "623   Audio_Speech_Actors_01-24/Actor_05/03-01-01-01...      1\n",
       "1942  Audio_Song_Actors_01-24/Actor_15/03-02-03-02-0...      3\n",
       "1331  Audio_Speech_Actors_01-24/Actor_05/03-01-08-02...      8\n",
       "401   Audio_Speech_Actors_01-24/Actor_20/03-01-02-02...      2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "141f951c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_16/03-01-08-02...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_20/03-01-08-02...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_17/03-02-05-02-0...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>Audio_Song_Actors_01-24/Actor_24/03-02-04-01-0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>Audio_Speech_Actors_01-24/Actor_13/03-01-04-02...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  label\n",
       "1794  Audio_Speech_Actors_01-24/Actor_16/03-01-08-02...      8\n",
       "1833  Audio_Speech_Actors_01-24/Actor_20/03-01-08-02...      8\n",
       "1488  Audio_Song_Actors_01-24/Actor_17/03-02-05-02-0...      5\n",
       "2267  Audio_Song_Actors_01-24/Actor_24/03-02-04-01-0...      4\n",
       "290   Audio_Speech_Actors_01-24/Actor_13/03-01-04-02...      4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b839998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Data:  1961\n",
      "Length of Validation Data:  491\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Training Data: \", len(training_data))\n",
    "print(\"Length of Validation Data: \", len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b88d1697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237037,) 48000\n",
      "(79013,) 16000\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf \n",
    "import torchaudio\n",
    "import librosa\n",
    "waveform, sr = sf.read(training_data.loc[0,\"path\"])\n",
    "print((waveform.shape),sr)\n",
    "waveform, sr = librosa.load(training_data.loc[0,\"path\"], sr=16000)\n",
    "print(waveform.shape, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "059c0a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max duration: 6.37306250 seconds\n",
      "sample rate: 16000.00\n",
      "Longest file: Audio_Song_Actors_01-24/Actor_22/03-02-02-02-02-01-22.wav\n"
     ]
    }
   ],
   "source": [
    "max_duration = 0\n",
    "longest_file = \"\"\n",
    "for path in df_combined['path']:\n",
    "    # full_path = os.path.join(base_dir, path)\n",
    "    if os.path.isfile(path):\n",
    "        waveform, sample_rate = librosa.load(path, sr=16000)\n",
    "        duration = waveform.shape[0] / sample_rate\n",
    "        if duration > max_duration:\n",
    "            max_duration = duration\n",
    "            longest_file = path\n",
    "    else:\n",
    "        print(f\"one file path is not available {path}\")\n",
    "        break\n",
    "\n",
    "print(f\"Max duration: {max_duration:.8f} seconds\")\n",
    "print(f\"sample rate: {sample_rate:.2f}\")\n",
    "print(f\"Longest file: {longest_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2bd6ea75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101969,) 16000\n"
     ]
    }
   ],
   "source": [
    "waveform, sr = librosa.load(\"Audio_Song_Actors_01-24/Actor_22/03-02-02-02-02-01-22.wav\", sr =16000)\n",
    "print(waveform.shape, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "244a9a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({16000: 2452})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "sample_rates = []\n",
    "for full_path in df_combined['path']:\n",
    "    # full_path = os.path.join(base_dir, path)\n",
    "    if os.path.isfile(full_path):\n",
    "        _, sr = librosa.load(full_path, sr=16000)\n",
    "        sample_rates.append(sr)\n",
    "\n",
    "# Count frequency of each sample rate\n",
    "rate_counts = Counter(sample_rates)\n",
    "print(rate_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f1db898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data, processor, max_length= 6.37306250*16000):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        audio_path = self.data.iloc[index][\"path\"]\n",
    "        label = self.data.iloc[index][\"label\"]\n",
    "\n",
    "        audio, sr = librosa.load(audio_path)\n",
    "        audio = audio.squeeze()\n",
    "\n",
    "        if len(audio) > self.max_length :\n",
    "            audio = audio[:self.max_length]\n",
    "            print(f\"found a audio file greater than max length : {audio_path}\")\n",
    "        else:\n",
    "            audio = np.pad(audio, (0,int(self.max_length-len(audio))), \"constant\")\n",
    "            \n",
    "        inputs = self.processor(audio, sampling_rate=16000, return_tensors='pt', padding=True, truncate=True, max_length=self.max_length)\n",
    "        input_values = inputs.input_values.squeeze()\n",
    "\n",
    "        return {'input_values': input_values, 'labels': torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "93642dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a6497061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, Wav2Vec2Config, Trainer, TrainingArguments\n",
    "config = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", num_labels=8)\n",
    "model = Wav2Vec2ForSequenceClassification(config)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "96b2ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(training_data, processor)\n",
    "validation_dataset = AudioDataset(validation_data, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3a77c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "83f876c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:131\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\akhilesh\\.conda\\envs\\akhilesh-env\\Lib\\site-packages\\transformers\\training_args.py:1738\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1736\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1738\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[32m   1741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.average_tokens_across_devices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\akhilesh\\.conda\\envs\\akhilesh-env\\Lib\\site-packages\\transformers\\training_args.py:2268\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2264\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2265\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2266\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2267\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\akhilesh\\.conda\\envs\\akhilesh-env\\Lib\\site-packages\\transformers\\utils\\generic.py:67\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     65\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\akhilesh\\.conda\\envs\\akhilesh-env\\Lib\\site-packages\\transformers\\training_args.py:2138\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2139\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2140\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2141\u001b[39m         )\n\u001b[32m   2142\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2143\u001b[39m accelerator_state_kwargs = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e9809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf12b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "akhilesh-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
